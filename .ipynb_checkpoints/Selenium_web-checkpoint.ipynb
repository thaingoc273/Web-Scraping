{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions which load html from page adrress\n",
    "\n",
    "# Load static web with BeautifulSoup\n",
    "def load_bs(url):\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        return BeautifulSoup(html, 'lxml')\n",
    "    except:\n",
    "        print('Loading fail')\n",
    "\n",
    "# Load dynamics web with Selenium and Browser type\n",
    "def driver_selenium(typ, url):\n",
    "    try:\n",
    "        if typ == 'Chrome':\n",
    "            driver = webdriver.Chrome(executable_path='C:\\\\Users\\\\Pham Thi Ngan\\\\Downloads\\\\Jupiter Lab\\\\Final Project\\\\driver\\\\chromedriver.exe')\n",
    "        elif typ == 'Firefox':\n",
    "            driver = webdriver.Firefox(executable_path='C:\\\\Users\\\\Pham Thi Ngan\\\\Downloads\\\\Jupiter Lab\\\\Final Project\\\\driver\\\\geckodriver.exe')\n",
    "        elif typ == 'Edge':\n",
    "            driver = webdriver.Edge(executable_path='C:\\\\Users\\\\Pham Thi Ngan\\\\Downloads\\\\Jupiter Lab\\\\Final Project\\\\driver\\\\msedgedriver.exe')\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "    except:\n",
    "        print('Loading driver fail')\n",
    "\n",
    "def html_selenium(driver):\n",
    "    try:    \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "    except:\n",
    "        print('Loading html fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define driver for web scraping\n",
    "def xing_scrap(job_title, location, typ):\n",
    "    url = 'https://www.xing.com/jobs'\n",
    "    driver = driver_selenium(typ, url)\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"consent-accept-button\"]').click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"xjm_keyword\"]').clear()\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"xjm_keyword\"]').send_keys(job_title)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"xjm_location\"]').clear()\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"xjm_location\"]').send_keys(location)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"maincontent\"]/div/div/section[1]/div[1]/div[2]/div/div/div/div/div/form/button').click()\n",
    "    time.sleep(1)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_page_loading(body_jobs):\n",
    "    lst_job = []\n",
    "    for job in body_jobs:\n",
    "        try:\n",
    "            link_detail = 'https://www.xing.com'+job.a['href']\n",
    "        except:\n",
    "            link_detail = None\n",
    "\n",
    "        job_detail = job.find('div', class_ = 'result-result-content-40c8b994')\n",
    "\n",
    "        try:\n",
    "            title = job_detail.h2.text\n",
    "        except:\n",
    "            title = None\n",
    "        try:\n",
    "            company = job_detail.find('div', class_='result-result-subtitle-3f97fea9').text\n",
    "        except:\n",
    "            company = None\n",
    "        try:\n",
    "            rating = job_detail.find('span', class_ = 'kununu-rating-kununu-rating-average-21e99cf7').text\n",
    "        except:\n",
    "            rating = None\n",
    "        try:\n",
    "            number_rate = job_detail.find('span', class_ = 'kununu-rating-kununu-rating-reviews-1546bf28').text\n",
    "        except:\n",
    "            number_rate = None\n",
    "        try:\n",
    "            time = job_detail.find('time')['datetime']\n",
    "            time_old = job_detail.find('time').text\n",
    "        except:\n",
    "            time = None\n",
    "        try:\n",
    "            description = job_detail.find('div', class_ = 'result-result-description-b544c1ff').text\n",
    "        except:\n",
    "            description = None\n",
    "        tupl = (title, company, location, rating, number_rate, time, time_old, description, link_detail)\n",
    "        lst_job.append(tupl)\n",
    "    return lst_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xing_jobs(title, location, typ):    \n",
    "    driver = xing_scrap(title, location, typ)\n",
    "    time.sleep(20)\n",
    "    lst_jobs = []\n",
    "    while True:\n",
    "        try:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            body_jobs = soup.find_all('div', {'class':'result-result-container-d527f6c7'})\n",
    "            add_jobs = one_page_loading(body_jobs)\n",
    "            lst_jobs = lst_jobs + add_jobs\n",
    "            time.sleep(10)\n",
    "            driver.find_element_by_xpath('//*[@id=\"content\"]/div/div/main/div[3]/div[22]').find_elements_by_tag_name('li')[-1].click()\n",
    "            time.sleep(40)\n",
    "        except:\n",
    "            break\n",
    "    # Save jobs to csv file\n",
    "    with open('xing_jobs.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        #writer.writerow(['JobTitle', 'Company', 'Location', 'Type_Work', 'Salary', 'PostDate', 'ExtractDate', 'Summary',  'JobUrl'])\n",
    "        writer.writerows(lst_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xing_jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "       writer = csv.writer(f)\n",
    "       writer.writerow(['JobTitle', 'Company', 'Location', 'Rating', 'Number_rate', 'PostDate', 'PostOld', 'Summary',  'JobUrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_webdriver =['Chrome', 'Firefox', 'Edge']\n",
    "title = 'Data Scientist'\n",
    "location = 'Berlin'\n",
    "typ = typ_webdriver[2]\n",
    "xing_jobs(title, location, typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDEED JOBS SCRAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "       writer = csv.writer(f)\n",
    "       writer.writerow(['Job_Title', 'Company', 'Location', 'Remote', 'Salary', 'Post_Date', 'Extract_Date', 'Summary',  'JobUrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(title, location):\n",
    "    return 'https://de.indeed.com/jobs?q={}&l={}&sort=date'.format(title, location)\n",
    "\n",
    "def one_page_indeed(jobs):\n",
    "    records = []\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            title = job.find('h2', class_='title').a.text.strip()\n",
    "        except:\n",
    "            title = None\n",
    "        try:\n",
    "            link_detail = 'https://de.indeed.com'+job.find('h2', {'class':'title'}).a['href']\n",
    "        except:\n",
    "            link_detail = None\n",
    "        try:\n",
    "            company = job.find('span', {'class':'company'}).text.strip()\n",
    "        except:\n",
    "            company = None\n",
    "        try:\n",
    "            location = job.find('span', class_ = 'location').text.strip()\n",
    "        except:\n",
    "            location = None\n",
    "        try:\n",
    "            remote = job.find('span', class_ = 'remote').text.strip()\n",
    "        except:\n",
    "            remote = None\n",
    "        try:\n",
    "            salary = job.find('span', class_ = 'salaryText').text.strip()\n",
    "        except:\n",
    "            salary = None\n",
    "        try:\n",
    "            date = job.find('span', class_ = 'date').text.strip()\n",
    "        except:\n",
    "            date = None\n",
    "        response_detail = requests.get(link_detail)\n",
    "        soup_detail = BeautifulSoup(response_detail.text, 'lxml')\n",
    "        try:\n",
    "            job_descrip = soup_detail.find('div', {'id':'jobDescriptionText'}).text.strip()\n",
    "        except:\n",
    "            job_descrip = None\n",
    "        record = (title, company, location, remote, salary, date, today, job_descrip, link_detail)\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "def indeed_jobs(title, location):\n",
    "    url = get_url(title, location)   \n",
    "    #job_list =[]\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        jobs = soup.find_all('div', {'class':'jobsearch-SerpJobCard'}) \n",
    "        records = one_page_indeed(jobs)\n",
    "        with open('indeed_jobs.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            #writer.writerow(['JobTitle', 'Company', 'Location', 'Type_Work', 'Salary', 'PostDate', 'ExtractDate', 'Summary',  'JobUrl'])\n",
    "            writer.writerows(records)    \n",
    "        try:\n",
    "            url = 'https://de.indeed.com'+soup.find('a', {'aria-label':\"Weiter\"}).get('href')\n",
    "        except:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "       writer = csv.writer(f)\n",
    "       writer.writerow(['JobTitle', 'Company', 'Location', 'Remote', 'Salary', 'PostDate', 'Extract_Date', 'Summary',  'JobUrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Data Analyst'\n",
    "location = 'Berlin'\n",
    "indeed_jobs(title, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
